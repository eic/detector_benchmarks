from itertools import product
import hashlib

DETECTOR_PATH = os.environ["DETECTOR_PATH"]
DETECTOR_VERSION = os.environ["DETECTOR_VERSION"]
SUBSYSTEM = "roman_pots"
BENCHMARK = "dense_neural_network"
DETECTOR_CONFIG = ["epic_craterlake", "epic_brycecanyon"]
NUM_TRAINING_INPUTS = [100,10]
NUM_EPOCHS_PZ = [10, 100, 1000]
LEARNING_RATE_PZ = [10,5]
SIZE_INPUT_PZ = [10,4]
SIZE_OUTPUT_PZ = [10,3]
N_LAYERS_PZ = [10,2]
SIZE_FIRST_HIDDEN_LAYER_PZ = [10]
MULTIPLIER_PZ = [10]
LEAK_RATE_PZ = [0.025]
NUM_EPOCHS_PY = [10, 100, 1000]
LEARNING_RATE_PY = [10]
SIZE_INPUT_PY = [10]
SIZE_OUTPUT_PY = [10]
N_LAYERS_PY = [10]
SIZE_FIRST_HIDDEN_LAYER_PY = [10]
MULTIPLIER_PY = [10]
LEAK_RATE_PY = [0.025]
NUM_EPOCHS_PX = [10, 100, 1000]
LEARNING_RATE_PX = [10]
SIZE_INPUT_PX = [10]
SIZE_OUTPUT_PX = [10]
N_LAYERS_PX = [10]
SIZE_FIRST_HIDDEN_LAYER_PX = [10]
MULTIPLIER_PX = [10]
LEAK_RATE_PX = [0.025]
MAX_HASH = 6
MODEL_VERSION = [
    hashlib.sha512("_".join(map(str,x)).encode()).hexdigest()[:MAX_HASH]
    for x in product(
        NUM_TRAINING_INPUTS,
        NUM_EPOCHS_PZ, LEARNING_RATE_PZ, SIZE_INPUT_PZ, SIZE_OUTPUT_PZ, N_LAYERS_PZ, SIZE_FIRST_HIDDEN_LAYER_PZ, MULTIPLIER_PZ, LEAK_RATE_PZ,
        NUM_EPOCHS_PY, LEARNING_RATE_PY, SIZE_INPUT_PY, SIZE_OUTPUT_PY, N_LAYERS_PY, SIZE_FIRST_HIDDEN_LAYER_PY, MULTIPLIER_PY, LEAK_RATE_PY,
        NUM_EPOCHS_PX, LEARNING_RATE_PX, SIZE_INPUT_PX, SIZE_OUTPUT_PX, N_LAYERS_PX, SIZE_FIRST_HIDDEN_LAYER_PX, MULTIPLIER_PX, LEAK_RATE_PX
    )
]
INPUT_STEERING_FILE = "steering_file.py"
NFILES = range(1,3)
NEVENTS_PER_FILE = 10

rule all:
  input:
    expand("results/"+str(DETECTOR_VERSION)+"/{detector_config}/detector_benchmarks/"+str(SUBSYSTEM)+"/"+str(BENCHMARK)+"/raw_data/"+str(DETECTOR_VERSION)+"_{detector_config}_{index}.edm4hep.root",
           detector_config=DETECTOR_CONFIG,
           index=NFILES),
    expand("results/"+str(DETECTOR_VERSION)+"/{detector_config}/detector_benchmarks/"+str(SUBSYSTEM)+"/"+str(BENCHMARK)+"/metadata/"+str(DETECTOR_VERSION)+"_{detector_config}_"+str(SUBSYSTEM)+"_"+str(BENCHMARK)+"_{model_version}.txt",
           detector_config=DETECTOR_CONFIG,
           model_version=MODEL_VERSION)


rule roman_pots_generate_events:
  input:
  output:
    expand("results/"+str(DETECTOR_VERSION)+"/{detector_config}/detector_benchmarks/"+str(SUBSYSTEM)+"/"+str(BENCHMARK)+"/raw_data/"+str(DETECTOR_VERSION)+"_{detector_config}_{index}.edm4hep.root",
           detector_config=DETECTOR_CONFIG,
           index=NFILES)
  run:
    for detector_config, index in product(DETECTOR_CONFIG, NFILES):
      os.system("npsim --steeringFile "+str(INPUT_STEERING_FILE)+" --compactFile "+str(DETECTOR_PATH)+"/"+str(detector_config)+".xml --outputFile results/"+str(DETECTOR_VERSION)+"/"+str(detector_config)+"/detector_benchmarks/"+str(SUBSYSTEM)+"/"+str(BENCHMARK)+"/raw_data/"+str(DETECTOR_VERSION)+"_"+str(detector_config)+"_"+str(index)+".edm4hep.root -N "+str(NEVENTS_PER_FILE))
    


rule roman_pots_generate_neural_network_configs:
  input:   
  output:
    expand("results/"+str(DETECTOR_VERSION)+"/{detector_config}/detector_benchmarks/"+str(SUBSYSTEM)+"/"+str(BENCHMARK)+"/metadata/"+str(DETECTOR_VERSION)+"_{detector_config}_"+str(SUBSYSTEM)+"_"+str(BENCHMARK)+"_{model_version}.txt",
           detector_config=DETECTOR_CONFIG,
           model_version=MODEL_VERSION)        
  run:
    for detector_config, num_training_inputs, \
    num_epochs_pz, learning_rate_pz, size_input_pz, size_output_pz, n_layers_pz, size_first_hidden_layer_pz, multiplier_pz, leak_rate_pz, \ 
    num_epochs_py, learning_rate_py, size_input_py, size_output_py, n_layers_py, size_first_hidden_layer_py, multiplier_py, leak_rate_py, \
    num_epochs_px, learning_rate_px, size_input_px, size_output_px, n_layers_px, size_first_hidden_layer_px, multiplier_px, leak_rate_px in \
    product(DETECTOR_CONFIG, NUM_TRAINING_INPUTS,
    NUM_EPOCHS_PZ, LEARNING_RATE_PZ, SIZE_INPUT_PZ, SIZE_OUTPUT_PZ, N_LAYERS_PZ, SIZE_FIRST_HIDDEN_LAYER_PZ, MULTIPLIER_PZ, LEAK_RATE_PZ,
    NUM_EPOCHS_PY, LEARNING_RATE_PY, SIZE_INPUT_PY, SIZE_OUTPUT_PY, N_LAYERS_PY, SIZE_FIRST_HIDDEN_LAYER_PY, MULTIPLIER_PY, LEAK_RATE_PY,
    NUM_EPOCHS_PX, LEARNING_RATE_PX, SIZE_INPUT_PX, SIZE_OUTPUT_PX, N_LAYERS_PX, SIZE_FIRST_HIDDEN_LAYER_PX, MULTIPLIER_PX, LEAK_RATE_PX): 
      output_dir = "results/"+str(DETECTOR_VERSION)+"/"+str(detector_config)+"/detector_benchmarks/"+str(SUBSYSTEM)+"/"+str(BENCHMARK)+"/metadata"  
      output_file = str(num_training_inputs)+"_"+\
                    str(num_epochs_pz)+"_"+str(learning_rate_pz)+"_"+str(size_input_pz)+"_"+str(size_output_pz)+"_"+str(n_layers_pz)+"_"+str(size_first_hidden_layer_pz)+"_"+str(multiplier_pz)+"_"+str(leak_rate_pz)+"_"+\
                    str(num_epochs_py)+"_"+str(learning_rate_py)+"_"+str(size_input_py)+"_"+str(size_output_py)+"_"+str(n_layers_py)+"_"+str(size_first_hidden_layer_py)+"_"+str(multiplier_py)+"_"+str(leak_rate_py)+"_"+\
                    str(num_epochs_px)+"_"+str(learning_rate_px)+"_"+str(size_input_px)+"_"+str(size_output_px)+"_"+str(n_layers_px)+"_"+str(size_first_hidden_layer_px)+"_"+str(multiplier_px)+"_"+str(leak_rate_px)
      output_file_location = open(str(output_dir)+"/"+str(DETECTOR_VERSION)+"_"+str(detector_config)+"_"+str(SUBSYSTEM)+"_"+str(BENCHMARK)+"_"+str(hashlib.sha512(output_file.encode()).hexdigest()[:MAX_HASH])+".txt","w")
      output_file_location.write(output_file)
      print(output_file_location)
      output_file_location.close()
          
